from dotenv import load_dotenv
load_dotenv(verbose=True)
# import os
# os.environ['ANTHROPIC_API_KEY'] = 'your-api-key'
# os.environ['TAVILY_API_KEY] = 'hogehoge'

import streamlit as st
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableConfig
from langchain.memory import ConversationBufferMemory
from langchain.agents import ConversationalChatAgent, AgentExecutor
from langchain_community.callbacks import StreamlitCallbackHandler
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

st.set_page_config(page_title="LangChain: Chat with search", page_icon="ğŸ¦œ")
st.title("ğŸ¦œ LangChain: Chat with search")

with st.sidebar:
    st.markdown("### Chat history")
    st.write("The chat history will be reset if there are no messages or if the button is pressed.")

# msgs :userã¨aiã®ä¼šè©±å±¥æ­´(ä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ã¯å«ã¾ãªã„)
# memory :ã‚ˆãã‚ã‹ã‚‰ã‚“ã€‚ã¨ã‚Šã‚ãˆãšmsgã‚’memoryã«ã„ã‚Œã€memoryã‚’agentexecuterã«æ¸¡ã—ã¦ãŠãã¨ã€agenteå®Ÿè¡Œæ™‚ã«msgã®å†…å®¹ãŒæ›´æ–°ã•ã‚Œã‚‹ã£ã½ã„ã€‚
msgs = StreamlitChatMessageHistory()
memory = ConversationBufferMemory(chat_memory=msgs, return_messages=True, memory_key="chat_history", output_key="output")

if len(msgs.messages) == 0 or st.sidebar.button("Reset chat history"):
    msgs.clear()
    # ã©ã†ã„ã†ã‚ã‘ã‹ã€æœ€åˆã‚’aiãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚ãªã®ã§ã€userãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æœ€åˆã«ã™ã‚‹ã€‚
    msgs.add_user_message("ã“ã‚“ã«ã¡ã¯ã€‚ã“ã‚Œã‹ã‚‰è³ªå•ã—ã¾ã™ã®ã§æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚")
    msgs.add_ai_message("æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚ä½•ã‚’èª¿ã¹ã¾ã™ã‹ï¼Ÿ")
    # ä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®è¾æ›¸ã‚’å®šç¾©(key:ä»˜å±ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, value:ä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ãƒªã‚¹ãƒˆ)
    st.session_state.intermediate_steps = {}


for idx, msg in enumerate(msgs.messages):
    # st.chat_messageã§ãƒ¦ãƒ¼ã‚¶ãƒ¼/AIã®å›ç­”ã‚’è¡¨ç¤ºã™ã‚‹
    with st.chat_message(msg.type): # user or assistant, https://docs.streamlit.io/develop/api-reference/chat/st.chat_message
        for intermediate_step in st.session_state.intermediate_steps.get(str(idx), []):
            agent_action, agent_output = intermediate_step
            # 01.ä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ã®å†…å®¹ã‚’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ³ãƒ†ãƒŠã«è¨˜è¼‰ã—ã¦ã€ã€ã€
            with st.status(f"**{agent_action.tool}**: {agent_action.tool_input}", state="complete"): # https://docs.streamlit.io/develop/api-reference/status/st.status
                st.write(agent_action.log)
                st.write(agent_output)
        # 02.ãã®ä¸‹ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜è¼‰ã™ã‚‹ã€‚
        st.write(msg.content)


if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    model = ChatAnthropic(streaming=True,model='claude-3-haiku-20240307')
    tools = [TavilySearchResults(max_results=1)]
    chat_agent = ConversationalChatAgent.from_llm_and_tools(llm=model, tools=tools)
    executor = AgentExecutor.from_agent_and_tools(
        agent=chat_agent,
        tools=tools,
        memory=memory,
        return_intermediate_steps=True,
        handle_parsing_errors=True,
    )
    with st.chat_message("assistant"):
        # ã“ã®è¾ºã‚‚ã‚ˆãã‚ã‹ã‚‰ã‚“ã€‚
        # æœ€çµ‚å›ç­”ã®ç›´å‰ã«ã€ŒComplete!ã€ã¨è¨˜è¼‰ã•ã‚ŒãŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ³ãƒ†ãƒŠãŒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã‚‚ã€ã“ã“ã®å‡¦ç†ã®ãŸã‚ã ã¨æ€ã†ãŒã€ã‚ˆãã‚ã‹ã‚‰ã‚“ã€‚
        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
        cfg = RunnableConfig()
        cfg["callbacks"] = [st_cb]
        response = executor.invoke(prompt, cfg)

        # æœ€çµ‚å›ç­”ã‚’è¡¨ç¤º
        st.write(response["output"])

        # ä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¿å­˜
        st.session_state.intermediate_steps[str(len(msgs.messages) - 1)] = response["intermediate_steps"]